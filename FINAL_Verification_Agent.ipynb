{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm8gO52nf2rO"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install slither-analyzer\n",
        "!solc-select install latest\n",
        "!solc-select use 0.8.25\n",
        "!pip install mythril"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y3y6ik-GNVC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84gzyjgXMeWH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhWOuj0byhDq"
      },
      "outputs": [],
      "source": [
        "# PRIVATE_KEY = \"\"\n",
        "client = OpenAI(\n",
        "    api_key=PRIVATE_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blo-BQiv8Ng1"
      },
      "outputs": [],
      "source": [
        "# helper functions to write and read files\n",
        "def write_to_file(filename, content):\n",
        "    with open(filename, \"w\") as file:\n",
        "        file.write(content)\n",
        "def read_file(filename):\n",
        "    with open(filename,'r') as file:\n",
        "        text = \" \".join(line.rstrip() for line in file)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Using GPT-3.5"
      ],
      "metadata": {
        "id": "GW-7IdK5RKRe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjhfZB4WZ85f"
      },
      "outputs": [],
      "source": [
        "# load zero-shot contracts from GPT-3.5\n",
        "gpt3_5 = pd.read_csv(\"/content/drive/MyDrive/ece473-final-project/ECE473 Final Project LLM Responses - GPT-3.5.csv\")\n",
        "\n",
        "total_trials = 10\n",
        "total_prompts = 1\n",
        "model = \"gpt3_5\"\n",
        "\n",
        "# dictionary to keep track of the number of iterations through Slither and Mythril\n",
        "num_iterations_slither = {}\n",
        "num_iterations_mythril = {}\n",
        "\n",
        "# the following is the prefix for all files generated\n",
        "# filename = model + \"_p\" + prompt + \"_t\" + trial + \"_\" + iteration\n",
        "\n",
        "# note: trials and prompts are 1-indexed\n",
        "# note: zero-shot contracts are identified with the prompt and trial number\n",
        "\n",
        "for j in range(total_prompts):\n",
        "    prompt = j + 1\n",
        "    num_iterations_slither[prompt] = []\n",
        "    num_iterations_mythril[prompt] = []\n",
        "\n",
        "    for i in range(total_trials):\n",
        "        trial = i + 1\n",
        "        iteration = 1\n",
        "        prev_code = \"\"\n",
        "        curr_code = gpt3_5[\"Trial Number \" + str(trial)][prompt-1]\n",
        "\n",
        "        # present model with current code\n",
        "        trial_1_chat = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"here is the current code: \" + curr_code,\n",
        "                }\n",
        "            ],\n",
        "\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "        )\n",
        "\n",
        "        # SLITHER\n",
        "        # break out of the loop if the same code is generated\n",
        "        while prev_code != curr_code:\n",
        "            file_pre = model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration)\n",
        "            old_filename = file_pre + \".sol\"\n",
        "            slither_filename = \"slither_\" + file_pre + \".txt\"\n",
        "\n",
        "            # make sure that there is already a file for the current code to feed into Slither\n",
        "            if (~os.path.isfile(old_filename)):\n",
        "                write_to_file(old_filename, curr_code)\n",
        "\n",
        "            # run Slither on current code\n",
        "            !slither {old_filename} 2> {slither_filename}\n",
        "\n",
        "            slither_output = read_file(slither_filename)\n",
        "\n",
        "            # stop if Slither gives the same feedback twice in a row\n",
        "            if iteration > 1:\n",
        "                prev_filename = \"slither_\" + model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".txt\"\n",
        "                if (~os.path.isfile(prev_filename)):\n",
        "                    prev_slither_output = read_file(prev_filename)\n",
        "                    if slither_output == prev_slither_output:\n",
        "                        break\n",
        "\n",
        "            prev_code = curr_code\n",
        "\n",
        "            # give the code back to GPT with comments to fix\n",
        "            trial_1_chat = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"fix the code: \" + curr_code + \"according to these comments: \" + slither_output + \" and PLEASE RETURN ONLY THE CODE. Reminder, the code starts with '// SPDX-License-Identifier: MIT'.\",\n",
        "                    }\n",
        "                ],\n",
        "\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "            )\n",
        "\n",
        "            iteration+=1\n",
        "            curr_code = trial_1_chat.choices[0].message.content\n",
        "\n",
        "            # make sure there is code from the new output and trim the output for just the code if necessary\n",
        "            start_index = curr_code.find(\"// SPDX\")\n",
        "            pragma_index = curr_code.find(\"pragma\")\n",
        "\n",
        "            if ((start_index == -1) and (pragma_index != -1)):\n",
        "                 start_index = pragma_index\n",
        "\n",
        "            # there is no code\n",
        "            if (start_index == -1):\n",
        "                # print(\"no code output at prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "\n",
        "                # this iteration does not have code, only count the last iteration (that has code)\n",
        "                iteration -= 1;\n",
        "                break\n",
        "            else:\n",
        "                # trim code from the front and back if necessary\n",
        "                if (start_index != 0):\n",
        "                    curr_code = curr_code[start_index:]\n",
        "\n",
        "                markdown_index = curr_code.rfind(\"```\")\n",
        "\n",
        "                if (markdown_index != -1):\n",
        "                    curr_code = curr_code[:markdown_index]\n",
        "\n",
        "        # keep track of the number of iterations through Slither for this zero-shot contract\n",
        "        num_iterations_slither[prompt].append(iteration)\n",
        "\n",
        "        print(\"Prompt \" + str(prompt) + \" (Slither) iterations: \" + str(iteration))\n",
        "\n",
        "\n",
        "        # print(\"starting mythril on prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "\n",
        "        # make sure the model has the most recent verison of the contract\n",
        "        curr_code = read_file(model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".sol\")\n",
        "\n",
        "        trial_1_chat = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"here is the current code: \" + curr_code,\n",
        "                }\n",
        "            ],\n",
        "\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "\n",
        "        )\n",
        "\n",
        "        # MYTHRIL\n",
        "        while prev_code != curr_code:\n",
        "            file_pre = model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1)\n",
        "            old_filename = file_pre + \".sol\"\n",
        "            mythril_filename = \"mythril_\" + file_pre + \".txt\"\n",
        "\n",
        "            # make sure that there is already a file for the current code to feed into Mythril\n",
        "            if (~os.path.isfile(old_filename)):\n",
        "                write_to_file(old_filename, curr_code)\n",
        "\n",
        "            # run Mythril on current code\n",
        "            !myth analyze {old_filename} 2> {mythril_filename}\n",
        "\n",
        "            mythril_output = read_file(mythril_filename)\n",
        "\n",
        "            # early stoppers for Mythril\n",
        "            if mythril_output == \"\" or len(mythril_output) < 1:\n",
        "                # print(\"no output from mythril at prompt  %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "                break\n",
        "            if (mythril_output == \"The analysis was completed successfully. No issues were detected.\"):\n",
        "                # print(\"no more errors found by mythril at iteration %d\" % iteration);\n",
        "                break;\n",
        "\n",
        "            # stop if Mythril gives the same feedback twice in a row\n",
        "            if iteration > num_iterations_slither[prompt][-1]:\n",
        "                prev_filename = \"mythril_\" + model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".txt\"\n",
        "                if (~os.path.isfile(prev_filename)):\n",
        "                    prev_mythril_output = read_file(prev_filename)\n",
        "                    if mythril_output == prev_mythril_output:\n",
        "                        break\n",
        "\n",
        "            prev_code = curr_code\n",
        "\n",
        "            # give the code back to GPT with comments to fix\n",
        "            trial_1_chat = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"fix the code: \" + curr_code + \"according to these comments: \" + mythril_output + \" and PLEASE RETURN ONLY THE CODE. Reminder, the code starts with '// SPDX-License-Identifier: MIT'.\",\n",
        "                    }\n",
        "                ],\n",
        "\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "            )\n",
        "\n",
        "            iteration+=1\n",
        "            curr_code = trial_1_chat.choices[0].message.content\n",
        "\n",
        "            # make sure there is code from the new output and trim the output for just the code if necessary\n",
        "            start_index = curr_code.find(\"// SPDX\")\n",
        "            pragma_index = curr_code.find(\"pragma\")\n",
        "\n",
        "            if ((start_index == -1) and (pragma_index != -1)):\n",
        "                 start_index = pragma_index\n",
        "\n",
        "            if (start_index == -1):\n",
        "                # print(\"no code output at prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "                iteration -= 1\n",
        "                break\n",
        "            else:\n",
        "                # trim code from front and back if necessary\n",
        "                if (start_index != 0):\n",
        "                    curr_code = curr_code[start_index:]\n",
        "\n",
        "                markdown_index = curr_code.rfind(\"```\")\n",
        "\n",
        "                if (markdown_index != -1):\n",
        "                    curr_code = curr_code[:markdown_index]\n",
        "\n",
        "        print(\"Prompt \" + str(prompt) + \" (Slither and Mythril) iterations: \" + str(iteration))\n",
        "\n",
        "        num_iterations_mythril[prompt].append(iteration)\n",
        "        i+=1\n",
        "        # print(\"next trial\")\n",
        "\n",
        "\n",
        "print(num_iterations_slither)\n",
        "print(num_iterations_mythril)\n",
        "\n",
        "!cp /content/gpt* drive/MyDrive/ece473-final-project/round2-gpt3_5\n",
        "!cp /content/slither* drive/MyDrive/ece473-final-project/round2-gpt3_5\n",
        "!cp /content/mythril* drive/MyDrive/ece473-final-project/round2-gpt3_5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Using GPT-4"
      ],
      "metadata": {
        "id": "vKe77ajGRDot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TipioE2NpYga"
      },
      "outputs": [],
      "source": [
        "# load zero-shot contracts from GPT4\n",
        "gpt4 = pd.read_csv(\"/content/drive/MyDrive/ece473-final-project/ECE473 Final Project LLM Responses - GPT-4.csv\")\n",
        "\n",
        "total_trials = 10\n",
        "total_prompts = 11\n",
        "model = \"gpt4\"\n",
        "\n",
        "# dictionary to keep track of the number of iterations through Slither and Mythril\n",
        "num_iterations_slither = {}\n",
        "\n",
        "# this includes both slither and mythril\n",
        "num_iterations_mythril = {}\n",
        "\n",
        "# the following is the prefix for all files generated\n",
        "# filename = model + \"_p\" + prompt + \"_t\" + trial + \"_\" + iteration\n",
        "\n",
        "# note: trials and prompts are 1-indexed\n",
        "# note: zero-shot contracts are identified with the prompt and trial number\n",
        "# note: it is costly and time consuming to run all 11 trials at once. change the number of prompts if needed\n",
        "\n",
        "for j in range(total_prompts):\n",
        "    prompt = j + 1\n",
        "    num_iterations_slither[prompt] = []\n",
        "    num_iterations_mythril[prompt] = []\n",
        "\n",
        "    for i in range(total_trials):\n",
        "        trial = i + 1\n",
        "        iteration = 1\n",
        "        prev_code = \"\"\n",
        "        curr_code = gpt4[\"Trial Number \" + str(trial)][prompt-1]\n",
        "\n",
        "        # present model with the zero-shot code\n",
        "        trial_1_chat = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"here is the current code: \" + curr_code,\n",
        "                }\n",
        "            ],\n",
        "\n",
        "            model=\"gpt-4\",\n",
        "        )\n",
        "\n",
        "        # SLITHER\n",
        "        while prev_code != curr_code:\n",
        "            file_pre = model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration)\n",
        "            old_filename = file_pre + \".sol\"\n",
        "            slither_filename = \"slither_\" + file_pre + \".txt\"\n",
        "            if (~os.path.isfile(old_filename)):\n",
        "                write_to_file(old_filename, curr_code)\n",
        "\n",
        "            # run slither on current code\n",
        "            !slither {old_filename} 2> {slither_filename}\n",
        "\n",
        "            slither_output = read_file(slither_filename)\n",
        "\n",
        "            # stop if Slither gives the same feedback twice in a row\n",
        "            if iteration > 1:\n",
        "                prev_filename = \"slither_\" + model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".txt\"\n",
        "                if (~os.path.isfile(prev_filename)):\n",
        "                    prev_slither_output = read_file(prev_filename)\n",
        "                    if slither_output == prev_slither_output:\n",
        "                        break\n",
        "\n",
        "            prev_code = curr_code\n",
        "\n",
        "            # give the code back to GPT with comments to fix\n",
        "            trial_1_chat = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"fix the code: \" + curr_code + \"according to these comments: \" + slither_output + \" and PLEASE RETURN ONLY THE CODE. Reminder, the code starts with '// SPDX-License-Identifier: MIT'.\",\n",
        "                    }\n",
        "                ],\n",
        "\n",
        "                model=\"gpt-4\",\n",
        "            )\n",
        "            iteration+=1\n",
        "            curr_code = trial_1_chat.choices[0].message.content\n",
        "\n",
        "            # make sure there is code from the new output\n",
        "            start_index = curr_code.find(\"// SPDX\")\n",
        "            pragma_index = curr_code.find(\"pragma\")\n",
        "\n",
        "            if ((start_index == -1) and (pragma_index != -1)):\n",
        "                 start_index = pragma_index\n",
        "\n",
        "            if (start_index == -1):\n",
        "                # print(\"no code output at prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "\n",
        "                # this iteration does not have code, only count the last iteration (that has code)\n",
        "                iteration -= 1;\n",
        "                break\n",
        "            else:\n",
        "                if (start_index != 0):\n",
        "                    curr_code = curr_code[start_index:]\n",
        "\n",
        "                markdown_index = curr_code.rfind(\"```\")\n",
        "\n",
        "                if (markdown_index != -1):\n",
        "                    curr_code = curr_code[:markdown_index]\n",
        "\n",
        "                if iteration == 15:\n",
        "                  # print(\"iteration upper bound reached for Slither\")\n",
        "                  break\n",
        "\n",
        "        num_iterations_slither[prompt].append(iteration)\n",
        "        print(\"Prompt \" + str(prompt) + \" (Slither) iterations: \" + str(iteration))\n",
        "\n",
        "        # print(\"starting mythril on prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "\n",
        "        curr_code = read_file(model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".sol\")\n",
        "\n",
        "        trial_1_chat = client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": \"here is the current code: \" + curr_code,\n",
        "                }\n",
        "            ],\n",
        "\n",
        "            model=\"gpt-4\",\n",
        "\n",
        "        )\n",
        "\n",
        "        # MYTHRIL\n",
        "        while prev_code != curr_code:\n",
        "            # print(\"mythril prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "            file_pre = model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1)\n",
        "            old_filename = file_pre + \".sol\"\n",
        "            mythril_filename = \"mythril_\" + file_pre + \".txt\"\n",
        "            if (~os.path.isfile(old_filename)):\n",
        "                write_to_file(old_filename, curr_code)\n",
        "\n",
        "            !myth analyze {old_filename} 2> {mythril_filename}\n",
        "\n",
        "            mythril_output = read_file(mythril_filename)\n",
        "\n",
        "            if mythril_output == \"\" or len(mythril_output) < 1:\n",
        "                # print(\"no output from mythril at prompt  %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "                break\n",
        "            if (mythril_output == \"The analysis was completed successfully. No issues were detected.\"):\n",
        "                # print(\"no more errors found by myrthil at iteration %d\" % iteration);\n",
        "                break;\n",
        "\n",
        "            # stop if Mythril gives the same feedback twice in a row\n",
        "            if iteration > num_iterations_slither[prompt][-1]:\n",
        "                prev_filename = \"mythril_\" + model + \"_p\" + str(prompt) + \"_t\" + str(trial) + \"_\" + str(iteration - 1) + \".txt\"\n",
        "                if (~os.path.isfile(prev_filename)):\n",
        "                    prev_mythril_output = read_file(prev_filename)\n",
        "                    if mythril_output == prev_mythril_output:\n",
        "                        break\n",
        "\n",
        "            prev_code = curr_code\n",
        "\n",
        "            trial_1_chat = client.chat.completions.create(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": \"fix the code: \" + curr_code + \"according to these comments: \" + mythril_output + \" and PLEASE RETURN ONLY THE CODE. Reminder, the code starts with '// SPDX-License-Identifier: MIT'.\",\n",
        "                    }\n",
        "                ],\n",
        "\n",
        "                model=\"gpt-4\",\n",
        "            )\n",
        "\n",
        "            iteration+=1\n",
        "            curr_code = trial_1_chat.choices[0].message.content\n",
        "\n",
        "            # make sure there is code from the new output and trim the output if necessary\n",
        "            start_index = curr_code.find(\"// SPDX\")\n",
        "            pragma_index = curr_code.find(\"pragma\")\n",
        "\n",
        "            if ((start_index == -1) and (pragma_index != -1)):\n",
        "                 start_index = pragma_index\n",
        "\n",
        "            if (start_index == -1):\n",
        "                # print(\"no code output at prompt %d, trial %d, iteration %d\" % (prompt, trial, iteration))\n",
        "\n",
        "                # this iteration does not have code, only count the last iteration (that has code)\n",
        "                iteration -= 1\n",
        "                break\n",
        "            else:\n",
        "                # trim from the front and back for the code if necessary\n",
        "                if (start_index != 0):\n",
        "                    curr_code = curr_code[start_index:]\n",
        "\n",
        "                markdown_index = curr_code.rfind(\"```\")\n",
        "\n",
        "                if (markdown_index != -1):\n",
        "                    curr_code = curr_code[:markdown_index]\n",
        "\n",
        "                # if max iterations reached, stop\n",
        "                if iteration == 15:\n",
        "                    break\n",
        "\n",
        "        num_iterations_mythril[prompt].append(iteration)\n",
        "        print(\"Prompt \" + str(prompt) + \" (Slither and Mythril) iterations: \" + str(iteration))\n",
        "        i+=1\n",
        "\n",
        "print(num_iterations_slither)\n",
        "print(num_iterations_mythril)\n",
        "\n",
        "!cp /content/gpt* drive/MyDrive/ece473-final-project/round2-gpt4\n",
        "!cp /content/slither* drive/MyDrive/ece473-final-project/round2-gpt4\n",
        "!cp /content/mythril* drive/MyDrive/ece473-final-project/round2-gpt4\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}